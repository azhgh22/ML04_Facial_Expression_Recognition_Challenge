# ML04_Facial_Expression_Recognition_Challenge

training set დავყავი შემდეგნაირად:
      70% - train
      15% - validation
      15% - test

run1.   პირველ ექპერიმენტში არანარი პრეპროცესინგი არ ხდება. უბრალოდ ავიღე შემდეგი არქიტექტურა:
  Conv2d(1,32, padding=1) -> ReLU -> Flatten -> Linear(48*48*32, 256) -> ReLU -> Linear
  საკმაოდ მარტივი მოდელია. შედეგებიდან გამომდინარე (train-ზე accuracy 47% test-ზე 40%) ჩანს რომ მოდელმა ვერ დაისწავლა საკმარისი ინფორმაცია.
თანაც რადგანაც არ ვიყენებთ bachnorm-ს ჰიპერ პარამეტრების მორგებასა და წონების ინიციალიზაციას დიდი მნიშვნელობა ენიჭება.

run2 ამის შემდეგ დავამატე ორი კონვოლუციური ლეიერი რათა მოდელს შესძელბოდა უფრო მეტის რამის სწავლა, და ასევე თითოეულის შემდეგ ჩავსვი batchnormalization ლეიერი, 
 internal covariance shift -ის პრობლემის მოსაგვარებლად. ეს ლეიერი იმასაც უზრუნველყოფს, რომ თავდაპირველ წონებს ისეთი დიდი გავლენა აღარ ექნებათ outputებზე, რადგან ისინი ყველთვის დანორმალიზდება N(0,1) განაწილებაზე. შედეგად gradient vanishing/exploding პრობლემაც აღარ იქნება. ერთადერთი პრობლემა ისაა, რომ აქტივაციად ვიყენებთ relu-ს,
 BatchNorm ლეიერები კი ჩასმულია LeRu-ს წინ. პრობლემა შეიძლება შეიქმნას თუ რელუს input-ის უმეტესობა უარყოფითი იქნება. თუმცა როგორც შედეგებიდან ჩანს ეს პრობლემა არ შექმნილა,მეტიც ძალიან დიდი ვარიაცია შეგვრჩა ხელთ. 
             train accuracy : 99%
             val accuracy : 47%
      ასევე train-loss -ის გრაფიკს თუ დავაკვირდებით loss ეპოქბის მიხედვით წრფივად მცირდება, რაც დიდი ალბათობით, learning rate-ის დაბალ მნიშვნელობაზე მეტყველებს.
      ცოტას გავზრდი
      ამჯერდა ვარიაციის შემცირებას შევეცდები რეგულარიზაციების დამატებით. (dropout, L2, pooling)

run3. ამჯერად შევეცადე lr-ის ტუნინგს, რათა loss ის შემცირება უფრო სწრაფი ყოფილიყო(წრფივი შემცირება არ გვინდა)
      ასევე დავამატე რეგულარიზაცია dropout-ის სახით p=0.5 ყოველი რელუ ლეიერის შემდეგ. თუმცა როგორც აღმოჩნა ასეთი ჩამატება ზედმეტია და დიდი ბაიასი მოგვცა.
      train accuracy - 30%
      validation accuracy - 30%
      
