# ML04_Facial_Expression_Recognition_Challenge

training set დავყავი შემდეგნაირად:
      70% - train
      15% - validation
      15% - test

run1.   პირველ ექპერიმენტში არანარი პრეპროცესინგი არ ხდება. უბრალოდ ავიღე შემდეგი არქიტექტურა:
  Conv2d(1,32, padding=1) -> ReLU -> Flatten -> Linear(48*48*32, 256) -> ReLU -> Linear
  საკმაოდ მარტივი მოდელია. შედეგებიდან გამომდინარე (train-ზე accuracy 47% test-ზე 40%) ჩანს რომ მოდელმა ვერ დაისწავლა საკმარისი ინფორმაცია.
თანაც რადგანაც არ ვიყენებთ bachnorm-ს ჰიპერ პარამეტრების მორგებასა და წონების ინიციალიზაციას დიდი მნიშვნელობა ენიჭება.

run2 ამის შემდეგ დავამატე ორი კონვოლუციური ლეიერი რათა მოდელს შესძელბოდა უფრო მეტის რამის სწავლა, და ასევე თითოეულის შემდეგ ჩავსვი batchnormalization ლეიერი, 
 internal covariance shift -ის პრობლემის მოსაგვარებლად. ეს ლეიერი იმასაც უზრუნველყოფს, რომ თავდაპირველ წონებს ისეთი დიდი გავლენა აღარ ექნებათ outputებზე, რადგან ისინი ყველთვის დანორმალიზდება N(0,1) განაწილებაზე. შედეგად gradient vanishing/exploding პრობლემაც აღარ იქნება. ერთადერთი პრობლემა ისაა, რომ აქტივაციად ვიყენებთ relu-ს,
 BatchNorm ლეიერები კი ჩასმულია LeRu-ს წინ. პრობლემა შეიძლება შეიქმნას თუ რელუს input-ის უმეტესობა უარყოფითი იქნება. თუმცა როგორც შედეგებიდან ჩანს ეს პრობლემა არ შექმნილა,მეტიც ძალიან დიდი ვარიაცია შეგვრჩა ხელთ. 
             train accuracy : 99%
             val accuracy : 47%
      ასევე train-loss -ის გრაფიკს თუ დავაკვირდებით loss ეპოქბის მიხედვით წრფივად მცირდება, რაც დიდი ალბათობით, learning rate-ის დაბალ მნიშვნელობაზე მეტყველებს.
      ცოტას გავზრდი
      ამჯერდა ვარიაციის შემცირებას შევეცდები რეგულარიზაციების დამატებით. (dropout, L2, pooling)

run3. ამჯერად შევეცადე lr-ის ტუნინგს, რათა loss ის შემცირება უფრო სწრაფი ყოფილიყო(წრფივი შემცირება არ გვინდა)
      ასევე დავამატე რეგულარიზაცია dropout-ის სახით p=0.5 ყოველი რელუ ლეიერის შემდეგ. თუმცა როგორც აღმოჩნა ასეთი ჩამატება ზედმეტია და დიდი ბაიასი მოგვცა.
      train accuracy - 30%
      validation accuracy - 30%

run4-5. ამ ჯერად შევეცადე რეგულარიზაციის ტუნინგს, ცვადე არამარტო კარგი dropout-ის შერჩევა, არამედ შემოვიტანე რეგულარიზაცია pooling ლეიერის სახით. ის ყოველ ჯერზე ამცირებს პარამეტრებს, რაც ამცირებს მოდელის კომპლექსურობას შესაბამისად ვარიაციასაც. მიუხედავად იმისა რომ ვარიაცია შემცირადა, validation accuracy 51%-ს ვერ აცდა.
მართალია bias-variance-ის გადმოსახედიდან ასე თუ ისე კარგი მოდელი გვაქვს(5%იანი სხვაობაა ტესტსა და ტრეინს შორის, ნუ ცოტა overffit გვაქვს). მაგრამ 51% accuracy კარგი არ არის. ამის მიზეზი შეიძლება იყოს ის, რომ მოდელი ვერ სწავლობს კარგ feature-ებს. გარდა ამისა ზოგიერთ კლასის მიმართ დატა დაუბალანსებელია. ამიტომ smoothing-ის გამოყენება ასეთ შემთხვევაში კარგი იქნებოდა. მაგრამ მთავარი კითხვაა როგორ დავასწავლოთ უფრო კარგი feature-ები.

run6. ამ გაშვებაზე, ვცდი data Augmentation ტექნიკების დამატებას. ისეთები როგორიცაა, სურათის წაჩოჩება კიდეებისკენ, rotate, random crop... ამით დატაც გამდიდრდება, ეს overfitting-საც შეამცირებს, და მოდელს მისცემს საშუალებას სურათებიდან უფრო კონკრეტულ feature-ებს დააკვირდეს. საინტერესო შედეგი მივიღეთ: დატა აუგმენტაციის დამატებამ 
მოგვცა დიდი ბაიასი 46% ტესტიც და ტრეინიც. ჩემი აზრით ეს ნიშნავს, რომ მოდელმა აუგმენტაციის შედეგად ბევრი კარგი რაღაც ისწავლა, თუმცა რადგან ბევრი რეგულარიზაცია გვქონდა მოდებული მივიღეთ ბიასი. ამიტომ ამჯერად რეგულარიზაცია უნდა შევამციროთ. 

run7-8. აქ შევეცადე რეგულაცრიზაციის შემცირების ხარჯზე ბაიასის შემცირებასა და ვარიაციის გაზრდას. საბოლოო ჯამში dropout მთლიანად მოვაშორე, pooling ლეიერების რაოდენობაც შევამცირე. თუმცა შედეგი იგივეა. ამიტომ ალბათ ქსელი სიღრმე უნდა გავზარდოთ და უფრო კომპლექსური გავხადოთ.

run9-10-11. როგორც ვთქვი გავზარდე მოდელის კომპლექსურობა, კონვოლუციური ლეიერების რაოდენობა გავზარდე 5-მდე, თითოეულს მოჰყვება bachnorm  და რამდენიმეს maxpool ლეიერი. გარდა ამის ტრენინგის ეპოქქების რაოდენობა გავზარდე 30-მდე. მე11 გაშვებაში relu-ს მაგივრად ვცადე tanh აქტივაცია. თუმცა საბოოლოო ჯამში რელუსგან დიდად განსხვავებული შედეგი არ მოუცია. ტრენინგის გრაფს რომ ჩავხედოთ რელუმ უფრო სწრაფად მიაღწია თავის მაქსიმუმს. ამიტომ შემდეგ გაშვებებში ისევ ReLu გამოვიყენებ.
ამ გაშვებებში cross-validation-ით ვცადე learning rate-ის დატუნინგება, maxpool ლეიერის შერჩევაა... 
            საბოოლოო შედეგი კი ასეთი გვაქვს :
                        training accuracy: 64%
                        validation accuracy 58%

run12-13. აქ გავზარდე მოდელის კომპლექსურობა. Flatten-ის შემდეგ დავამატე სხვა ლეიერებიც. რამაც მოდელიც ტრენინგ სქორი გაზარდა. თუმცა ასევე გავზარდე რეგულარიზაციაც dropout-ის სახით. maxpool(3,3, strade = 2) ლეიერი გავხადე overlaped. pool-ის მეზობელ კვადრატებს ერთმანეთის შესახებ ექნებათ ინფორმაცია და გაითვალისწინებენ. .თუმცა მთავარი გავლენა იქონია Learning Scheduler-მა. წინა რანების training loss-ის გრაფს თუ დავაკვირდებით, შევამჩნევთ, რომ გრაფი ძალიან დახტუნავდა ზემოთ ქვემოთ, მიუხედავად იმისა რომ ჯამური ტრენდი ნორმალური ქონდა. ამის უაყოფითი შედეგი ის იყო, რომ მართალია საბოლოო ჯამში loss მცირდებოდა, მაგრამ ერთ გაშვების 30-ე ეპოქაზე შეიძლება ყოფილიყო 0.4, მეორეზე 0.6 მოკლედ არასტაბილური იყო. learning scheduler -ი კი learning rate-ს ამცირებს ექპსონენციალურად. თავიდან შეგვიძლია უფრო მკვეთრი ნაბიჯებით ვიმოძრათ. მაგრამ როდესაც მივუახლოვდებით convergence-ის ადგილს, ანუ მივუახლოვდებით მინიმუმს დიდი ნაბიჯის გამო მინიმუმიდან არ უნდა ამოვვარდეთ. სწრედ ეს უზრუნვეყო schedulerma. საბოოლოდ მოგვცა:
                                          training accuracy 67%
                                          validation accuracy 61%

run14. მცირედი overffit მაინც გვაქვს, ამიტომ მის გამოსწორებას შევეცდები L2- რეგულარიზაციით. ასევე Loss ფუნქციას დავუმატე class-ის წონები. ზოგიერთი კლასი ძალიან ცოტაა წარმოდგენილი. დაუბალანსებლობის ეფექტის აღმოსაფხვრელად კარგი ვარიანტია.

14 რუნში ვალიდაცია 61% ზე მაღლა ვერ ავიდა. შეიძლება იმიტომ რომ მხოლოდ 30 ეპოქაზე გავუშვი და ტრენინგი დააკლდა, შეიძლება იმიტომ რომ საკმარისად კომპლექსური მოდელი არ გვაქვს. თუმცა მოდელს რაც უფრო ვაკომპლექსურებთ, მით უფრო დიდია vanishing/exploding გრადიენტის პრობელმის რისკი. ასევე როგორც ResNet-ის ფეიფერშია განხილული ნეირონული ქსელის სიღრმის ზრდაასთან ერთად accuracy რაღაც ზღვარზე saturated ხდება. არადა ღრმა მოდელი, შესაბამისი რეგულარიზაციით კარგ feature-ებს აღმოაჩენდა. ამ მიზეზითა და მიზნებით გავტესტე ResNet. 

      მიუხედავად იმისა რომ ბევრი ლეიერი მაქვს, მაინც არ დადო უკეთესი შედეგი.
