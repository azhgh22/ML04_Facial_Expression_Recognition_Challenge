{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "collapsed_sections": [
        "ZmxAwYOhK__u",
        "tStVd46sSu4r",
        "hKR3TOzDpKvU"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "ZmxAwYOhK__u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zKus9_rJWpy"
      },
      "outputs": [],
      "source": [
        "!pip install kaggle wandb onnx -Uq\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ~/.kaggle\n",
        "!echo \"{\\\"username\\\":\\\"dachisuramelashvili\\\",\\\"key\\\":\\\"4202ec60e20b612a9947450bb8aeebb5\\\"}\" > ~/.kaggle\n",
        "!cp /content/drive/MyDrive/ML/kaggle.json ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "qmcp7QrlKDvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c challenges-in-representation-learning-facial-expression-recognition-challenge\n",
        "!unzip challenges-in-representation-learning-facial-expression-recognition-challenge.zip"
      ],
      "metadata": {
        "id": "TR3ldCi2Jefg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch # Main PyTorch Library\n",
        "from torch import nn # Used for creating the layers and loss function\n",
        "from torch.optim import Adam # Adam Optimizer\n",
        "import torchvision.transforms as transforms # Transform function used to modify and preprocess all the images\n",
        "from torch.utils.data import Dataset, DataLoader # Dataset class and DataLoader for creating the objects\n",
        "from sklearn.preprocessing import LabelEncoder # Label Encoder to encode the classes from strings to numbers\n",
        "import matplotlib.pyplot as plt # Used for visualizing the images and plotting the training progress\n",
        "from PIL import Image # Used to read the images from the directory\n",
        "import pandas as pd # Used to read/create dataframes (csv) and process tabular data\n",
        "import numpy as np # preprocessing and numerical/mathematical operations\n",
        "import os # Used to read the images path from the directory\n",
        "import random\n",
        "import wandb\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # detect the GPU if any, if not use CPU, change cuda to mps if you have a mac\n",
        "\n",
        "print(\"Device available: \", device)"
      ],
      "metadata": {
        "id": "dNV4a9vvK4Ke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Data"
      ],
      "metadata": {
        "id": "tStVd46sSu4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"train.csv\")"
      ],
      "metadata": {
        "id": "CpUnQzvhPfHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "6FynsR0dQz0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "def display_faces(image_data_list, num_images_to_display, image_shape=(48, 48)):\n",
        "  \"\"\"\n",
        "  Displays a specified number of images side by side from a list of image data.\n",
        "\n",
        "  Args:\n",
        "    image_data_list: A list of image data (e.g., pixel strings or NumPy arrays).\n",
        "    num_images_to_display: The number of images to display.\n",
        "    image_shape: A tuple representing the shape of each image (height, width).\n",
        "  \"\"\"\n",
        "  image_data_list = image_data_list['pixels'].tolist()\n",
        "\n",
        "  # Ensure we don't try to display more images than available\n",
        "  actual_images_to_display = min(num_images_to_display, len(image_data_list))\n",
        "\n",
        "  print(f\"Attempting to display {num_images_to_display} images.\")\n",
        "  if actual_images_to_display < num_images_to_display:\n",
        "      print(f\"Warning: Only {actual_images_to_display} images available in the list.\")\n",
        "\n",
        "  plt.figure(figsize=(actual_images_to_display * 3, 3))\n",
        "\n",
        "  for i in range(actual_images_to_display):\n",
        "    plt.subplot(1, actual_images_to_display, i + 1)\n",
        "\n",
        "    image_data = image_data_list[i]\n",
        "\n",
        "    if isinstance(image_data, str):\n",
        "      pixel_list = [int(pixel) for pixel in image_data.split()]\n",
        "      image_array = np.array(pixel_list).reshape(image_shape)\n",
        "    elif isinstance(image_data, np.ndarray):\n",
        "      image_array = image_data.reshape(image_shape)\n",
        "    else:\n",
        "      print(f\"Warning: Unsupported image data type at index {i}. Skipping.\")\n",
        "      continue\n",
        "\n",
        "    image = Image.fromarray(image_array.astype(np.uint8))\n",
        "\n",
        "    plt.imshow(image, cmap='gray')\n",
        "    plt.axis('off')\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "BCwXGVyxRqqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_faces(data, 15)"
      ],
      "metadata": {
        "id": "qhb4P0rlRQFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overfit_data=data[:16]\n",
        "train=data.sample(frac=0.7,random_state=42)\n",
        "val=data.drop(train.index).sample(frac=0.5, random_state=42)\n",
        "test=data.drop(train.index).drop(val.index)"
      ],
      "metadata": {
        "id": "7sMWXO7ElxI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overfit_data.shape, train.shape , val.shape , test.shape"
      ],
      "metadata": {
        "id": "fQ9bFSvFmEa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "def compute_mean_image(dataframe, image_shape=(48, 48)):\n",
        "  \"\"\"\n",
        "  Computes the mean image from a DataFrame containing image pixel data.\n",
        "\n",
        "  Args:\n",
        "    dataframe: A pandas DataFrame with a 'pixels' column containing space-separated\n",
        "               string representations of pixel values.\n",
        "    image_shape: A tuple representing the shape of each image (height, width).\n",
        "\n",
        "  Returns:\n",
        "    A NumPy array representing the mean image.\n",
        "  \"\"\"\n",
        "  all_images = []\n",
        "  for pixel_string in dataframe['pixels']:\n",
        "    print(pixel_string)\n",
        "    pixel_list = [int(pixel) for pixel in pixel_string.split()]\n",
        "    image_array = np.array(pixel_list).reshape(image_shape)\n",
        "    all_images.append(image_array)\n",
        "\n",
        "  mean_image = np.mean(all_images, axis=0)\n",
        "  return mean_image\n",
        "\n",
        "mean_image = compute_mean_image(train)\n",
        "\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.imshow(mean_image.astype(np.uint8), cmap='gray')\n",
        "plt.title('Mean Image')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "I4H87NoQXxOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ATgtwrppYFeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess"
      ],
      "metadata": {
        "id": "zYzQCsAeloYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])"
      ],
      "metadata": {
        "id": "qevE-vUBkhQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "class FacialExpressionDataset(Dataset):\n",
        "    def __init__(self, dataframe, transform=None):\n",
        "        self.dataframe = dataframe\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_pixels = self.dataframe.iloc[idx]['pixels']\n",
        "        label = int(self.dataframe.iloc[idx]['emotion'])  # still int here\n",
        "        label = torch.tensor(label, dtype=torch.long)     # convert to tensor\n",
        "\n",
        "        pixel_list = [int(pixel) for pixel in img_pixels.split()]\n",
        "        image_array = np.array(pixel_list).reshape(48, 48).astype(np.uint8)\n",
        "        image = Image.fromarray(image_array, mode='L')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image).to(device)  # keep on CPU\n",
        "\n",
        "        return image, label\n"
      ],
      "metadata": {
        "id": "mHEhV5-eSeKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_loader(dataframe, transform, batch_size=16):\n",
        "  dataset = FacialExpressionDataset(dataframe, transform)\n",
        "  loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "  return loader"
      ],
      "metadata": {
        "id": "t6jzdRBXXOu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_image(image):\n",
        "  plt.imshow(image.squeeze(0).numpy(), cmap='gray')\n",
        "  plt.title(\"Mean Image\")\n",
        "  plt.axis(\"off\")\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "q-nxGUseeTh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = FacialExpressionDataset(data, transform)"
      ],
      "metadata": {
        "id": "uaYahnHwjPgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.__getitem__(3)"
      ],
      "metadata": {
        "id": "3HOMS7SPjU6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "GJsv59RxnLu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseNN(nn.Module):\n",
        "  def __init__(self, hidden_dims: list[int] = [], normalization: str = '', dropout: float = 0):\n",
        "    super().__init__()\n",
        "\n",
        "    dims = [48*48] + hidden_dims + [7]\n",
        "\n",
        "    l = len(dims)\n",
        "    self.layers = nn.ModuleList()\n",
        "    self.layers.append(nn.Flatten())\n",
        "    for i in range(l-2):\n",
        "      self.layers.append(nn.Linear(dims[i], dims[i+1]))\n",
        "      if normalization == 'batch':\n",
        "        self.layers.append(nn.BatchNorm1d(dims[i+1]))\n",
        "      elif normalization == 'layer':\n",
        "        self.layers.append(nn.LayerNorm(dims[i+1]))\n",
        "      self.layers.append(nn.ReLU())\n",
        "      if dropout > 0:\n",
        "        self.layers.append(nn.Dropout(dropout))\n",
        "    self.layers.append(nn.Linear(dims[l-2], dims[l-1]))\n",
        "\n",
        "  def forward(self, x):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "gmP5gFsqmvgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(config):\n",
        "  return BaseNN(config['hidden_dims'], config['normalization'], config['dropout']).to(device)"
      ],
      "metadata": {
        "id": "VHI3NRuRbK5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    'learning_rate': 1e-3,\n",
        "    'epochs': 10,\n",
        "    'batch_size': 16,\n",
        "    'hidden_dims': [1024],\n",
        "    'normalization': 'none',\n",
        "    'dropout': 0.0,\n",
        "    'architecture': 'Simple NN'\n",
        "}"
      ],
      "metadata": {
        "id": "zb9J7ECE1jlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model(config)"
      ],
      "metadata": {
        "id": "en2IuKUrbPsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "summary(model, input_size = (48*48, ), device=device)"
      ],
      "metadata": {
        "id": "Fa__Zq1Lm_I-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Definitions"
      ],
      "metadata": {
        "id": "hKR3TOzDpKvU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "\n",
        "def train_model(model, data, criterion, optimizer, num_epochs=10, device='cuda', print_report=True, log_wandb=False):\n",
        "  train_loader = data['train']\n",
        "  val_loader = data['val']\n",
        "\n",
        "  history = {\n",
        "      'train_loss': [],\n",
        "      'val_loss': [],\n",
        "      'train_acc': [],\n",
        "      'val_acc': [],\n",
        "      'grad_to_weight_ratio': [],\n",
        "      'weight_norm': []\n",
        "  }\n",
        "\n",
        "  model.to(device)\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    start_time = time.time()\n",
        "\n",
        "    # ---- TRAINING ----\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    total_grad = 0.0\n",
        "    total_weight = 0.0\n",
        "    current_grad_ratio = 0.0\n",
        "    total_grad_ratio = []\n",
        "    avg_grad_ratio = 0.0\n",
        "\n",
        "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "      # Forward/Backward\n",
        "      images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(images)\n",
        "      loss = criterion(outputs, labels)\n",
        "      loss.backward()\n",
        "\n",
        "      # Grad ration calculation\n",
        "      for param in model.parameters():\n",
        "        if param.grad is not None:\n",
        "          total_grad += param.grad.norm().item()\n",
        "          total_weight += param.data.norm().item()\n",
        "\n",
        "      if total_weight > 0:\n",
        "        current_grad_ratio = (total_grad / total_weight) * 100\n",
        "      else:\n",
        "        current_grad_ratio = 0.0\n",
        "\n",
        "      total_grad_ratio.append(current_grad_ratio)\n",
        "\n",
        "      # Update\n",
        "      optimizer.step()\n",
        "\n",
        "      running_loss += loss.item() * images.size(0)\n",
        "      _, predicted = outputs.max(1)\n",
        "      total += labels.size(0)\n",
        "      correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "      if print_report and batch_idx % 200 == 0:\n",
        "          print(f\"[Epoch {epoch+1}/{num_epochs}] \"\n",
        "                f\"Batch {batch_idx}/{len(train_loader)} - \"\n",
        "                f\"Loss: {loss.item():.4f} - \"\n",
        "                f\"Processed: {total} images - \"\n",
        "                f\"Grad_to_weight_ratio: {current_grad_ratio:.2%}\")\n",
        "      if print_report and batch_idx % 400 == 0:\n",
        "        # Print what is happening to grads\n",
        "        for i, layer in enumerate(model.layers):\n",
        "          if hasattr(layer, 'weight'):\n",
        "            grad = layer.weight.grad\n",
        "            if grad is not None:\n",
        "              print(f\"Layer {i} ({layer}): grad mean = {grad.mean().item():.6f}\")\n",
        "            else:\n",
        "              print(f\"Layer {i} ({layer}): grad = None\")\n",
        "\n",
        "\n",
        "    train_loss = running_loss / total\n",
        "    train_acc = correct / total\n",
        "    avg_grad_ratio = sum(total_grad_ratio) / len(total_grad_ratio)\n",
        "\n",
        "    # ---- VALIDATION ----\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for images, labels in val_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        val_loss += loss.item() * images.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        val_total += labels.size(0)\n",
        "        val_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    epoch_time = time.time() - start_time\n",
        "\n",
        "    val_loss /= val_total\n",
        "    val_acc = val_correct / val_total\n",
        "\n",
        "    weight_norm = sum(p.data.norm().item() for p in model.parameters())\n",
        "\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['val_acc'].append(val_acc)\n",
        "    history['grad_to_weight_ratio'].append(avg_grad_ratio)\n",
        "    history['weight_norm'].append(weight_norm)\n",
        "\n",
        "    if print_report:\n",
        "      print(f\"\\nEpoch [{epoch+1}/{num_epochs}] completed in {epoch_time:.1f}s\")\n",
        "      print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2%}\")\n",
        "      print(f\"Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc:.2%}\")\n",
        "      print(f\"Grad to Weight Ratio: {avg_grad_ratio:.2%}\")\n",
        "      print(f\"Weight Norm: {weight_norm:.2f}\\n\")\n",
        "    if log_wandb:\n",
        "      metrics = {\n",
        "          f\"epoch\": epoch + 1,\n",
        "          f\"train_loss\": train_loss,\n",
        "          f\"val_loss\": val_loss,\n",
        "          f\"train_acc\": train_acc,\n",
        "          f\"val_acc\": val_acc,\n",
        "          f\"grad_ratio\": avg_grad_ratio,\n",
        "          f\"weight_norm\": weight_norm,\n",
        "          f\"epoch_time\": epoch_time\n",
        "      }\n",
        "\n",
        "      if epoch > 0:\n",
        "          loss_data = [[x, history['train_loss'][x], history['val_loss'][x]]\n",
        "                     for x in range(epoch + 1)]\n",
        "          acc_data = [[x, history['train_acc'][x], history['val_acc'][x]]\n",
        "                     for x in range(epoch + 1)]\n",
        "\n",
        "          metrics.update({\n",
        "              f\"loss_plot\": wandb.plot.line_series(\n",
        "                  xs=range(epoch + 1),\n",
        "                  ys=[history['train_loss'], history['val_loss']],\n",
        "                  keys=[\"Train\", \"Val\"],\n",
        "                  title=\"Loss Progress\",\n",
        "                  xname=\"Epoch\"\n",
        "              ),\n",
        "              f\"acc_plot\": wandb.plot.line_series(\n",
        "                  xs=range(epoch + 1),\n",
        "                  ys=[history['train_acc'], history['val_acc']],\n",
        "                  keys=[\"Train\", \"Val\"],\n",
        "                  title=\"Accuracy Progress\",\n",
        "                  xname=\"Epoch\"\n",
        "              ),\n",
        "              \"grad_plot\": wandb.plot.line(\n",
        "                  wandb.Table(\n",
        "                      data=[[x, y] for x, y in zip(range(epoch + 1), history['grad_to_weight_ratio'])],\n",
        "                      columns=[\"Epoch\", \"Grad/Weight Ratio\"]\n",
        "                  ),\n",
        "                  \"Epoch\",  # x-axis\n",
        "                  \"Grad/Weight Ratio\",  # y-axis\n",
        "                  title=\"Gradient-to-Weight Ratio Over Time\"\n",
        "              )\n",
        "          })\n",
        "\n",
        "      wandb.log(metrics)\n",
        "\n",
        "\n",
        "  return history\n"
      ],
      "metadata": {
        "id": "02tmgUpgpUDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "WmiAy9eJep3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make(config):\n",
        "  model = get_model(config)\n",
        "  optimizer = Adam(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  training_data = {\n",
        "      'train': to_loader(train, transform=transform, batch_size=config['batch_size']),\n",
        "      'val': to_loader(val, transform=transform, batch_size=config['batch_size']),\n",
        "      'test': to_loader(test, transform=transform, batch_size=config['batch_size'])\n",
        "  }\n",
        "\n",
        "  return model, training_data, criterion, optimizer\n"
      ],
      "metadata": {
        "id": "jKlFY0KjCn3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def overfit(config):\n",
        "  model, data, criterion, optimizer = make(config)\n",
        "\n",
        "  overfit_data = {\n",
        "      'train': to_loader(train[:16], transform=transform, batch_size=config['batch_size']),\n",
        "      'val': to_loader(val[:16], transform=transform, batch_size=config['batch_size'])\n",
        "  }\n",
        "\n",
        "  return train_model(model, overfit_data, criterion, optimizer, 100, device, False, False)"
      ],
      "metadata": {
        "id": "XEmsh4w755En"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overfit_history = overfit(config)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "4aTYfb4OWu8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, training_data, criterion, optimizer = make(config)\n",
        "\n",
        "history = train_model(model, training_data, criterion, optimizer, 3, device, True)"
      ],
      "metadata": {
        "id": "pV6xzArcpd2h",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing Definitions"
      ],
      "metadata": {
        "id": "3YK3ZWXWIH8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, test_loader, criterion=None, device='cpu', print_report=True, log_wandb=False):\n",
        "    \"\"\"\n",
        "    Improved version with zero_division handling and additional safeguards.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    total_loss = 0.0\n",
        "    true_labels = []\n",
        "    pred_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            if criterion is not None:\n",
        "                loss = criterion(outputs, labels)\n",
        "                total_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            _, predictions = torch.max(outputs, 1)\n",
        "            total_correct += (predictions == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "            pred_labels.extend(predictions.cpu().numpy())\n",
        "\n",
        "    accuracy = total_correct / total_samples\n",
        "    results = {\n",
        "        'accuracy': accuracy,\n",
        "        'true_labels': np.array(true_labels),\n",
        "        'pred_labels': np.array(pred_labels)\n",
        "    }\n",
        "\n",
        "    if criterion is not None:\n",
        "        results['loss'] = total_loss / total_samples\n",
        "\n",
        "    present_labels = np.unique(true_labels)\n",
        "    clf_report = classification_report(\n",
        "        true_labels, pred_labels,\n",
        "        labels=present_labels,\n",
        "        zero_division=0,\n",
        "        output_dict=True\n",
        "    )\n",
        "\n",
        "    if log_wandb:\n",
        "        wandb.log(results)\n",
        "\n",
        "        for label in present_labels:\n",
        "            label = str(label)\n",
        "            wandb.log({\n",
        "                f'test/precision_{label}': clf_report[label]['precision'],\n",
        "                f'test/recall_{label}': clf_report[label]['recall'],\n",
        "                f'test/f1_{label}': clf_report[label]['f1-score'],\n",
        "            })\n",
        "\n",
        "        wandb.log({\n",
        "            \"confusion_matrix\": wandb.plot.confusion_matrix(\n",
        "                y_true=true_labels,\n",
        "                preds=pred_labels,\n",
        "                class_names=[str(x) for x in present_labels]\n",
        "            )\n",
        "        })\n",
        "\n",
        "    if print_report:\n",
        "        print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "        if criterion is not None:\n",
        "            print(f\"Test Loss: {results['loss']:.4f}\")\n",
        "\n",
        "        # Handle cases where some classes are missing in predictions\n",
        "        present_labels = np.unique(true_labels)\n",
        "        print(\"\\nClassification Report (subset of classes present in test set):\")\n",
        "        print(classification_report(\n",
        "            true_labels,\n",
        "            pred_labels,\n",
        "            labels=present_labels,\n",
        "            zero_division=0  # Silences the warning by defining 0/0 = 0\n",
        "        ))\n",
        "\n",
        "        print(\"\\nConfusion Matrix (subset):\")\n",
        "        print(confusion_matrix(true_labels, pred_labels, labels=present_labels))\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "NEGt2zc3INo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_model(model, training_data['test'], criterion, device)"
      ],
      "metadata": {
        "id": "bZd0eqrVJGuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot"
      ],
      "metadata": {
        "id": "s2RaE1Tfsrxd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_training_history(history, is_overfit=False):\n",
        "    \"\"\"\n",
        "    Plots training and validation loss, accuracy, gradient-to-weight ratio, and weight norm.\n",
        "    \"\"\"\n",
        "    epochs = range(1, len(history['train_loss']) + 1)\n",
        "\n",
        "    plt.figure(figsize=(24, 5))\n",
        "\n",
        "    # ---- LOSS ----\n",
        "    plt.subplot(1, 4, 1)\n",
        "    plt.plot(epochs, history['train_loss'], label='Train Loss', marker='o')\n",
        "    plt.plot(epochs, history['val_loss'], label='Val Loss', marker='o')\n",
        "    plt.title('Loss over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # ---- ACCURACY ----\n",
        "    plt.subplot(1, 4, 2)\n",
        "    plt.plot(epochs, history['train_acc'], label='Train Acc', marker='o')\n",
        "    plt.plot(epochs, history['val_acc'], label='Val Acc', marker='o')\n",
        "    plt.title('Accuracy over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # ---- GRADIENT-TO-WEIGHT RATIO ----\n",
        "    plt.subplot(1, 4, 3)\n",
        "    plt.plot(epochs, history['grad_to_weight_ratio'], label='Grad/Weight Ratio', marker='o', color='purple')\n",
        "    plt.title('Grad-to-Weight Ratio')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Ratio (%)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # ---- WEIGHT NORM ----\n",
        "    plt.subplot(1, 4, 4)\n",
        "    plt.plot(epochs, history['weight_norm'], label='Weight Norm', marker='o', color='darkgreen')\n",
        "    plt.title('Weight Norm over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Norm (L2)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"training_plot{'_overfit' if is_overfit else ''}.png\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "dKP47yqFZWXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training_history(overfit_history)"
      ],
      "metadata": {
        "id": "vPLIij4OJgvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Pipeline"
      ],
      "metadata": {
        "id": "Nnr_FxgotiG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()"
      ],
      "metadata": {
        "id": "_Go3qdTsuXR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    'learning_rate': 1e-3,\n",
        "    'weight_decay': 0,\n",
        "    'epochs': 10,\n",
        "    'batch_size': 256,\n",
        "    'hidden_dims': [1024],\n",
        "    'normalization': 'none',\n",
        "    'dropout': 0.0,\n",
        "    'architecture': '1-1024-Layer-NN'\n",
        "}"
      ],
      "metadata": {
        "id": "on3OPzaA0Yrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with wandb.init(project='emotion-recognition', config=config, name=config['architecture']+''):\n",
        "  overfit_history = overfit(config)\n",
        "  plot_training_history(overfit_history, True)\n",
        "\n",
        "  wandb.log({\"Overfit Training Curves\": wandb.Image(\"training_plot_overfit.png\")})\n",
        "\n",
        "  print('\\ntraining\\n')\n",
        "\n",
        "  model, training_data, criterion, optimizer = make(config)\n",
        "  summary(model, input_size = (48, 48, ), device=device)\n",
        "\n",
        "  wandb.watch(model, criterion, log='all', log_freq=10)\n",
        "  history = train_model(model, training_data, criterion, optimizer, config['epochs'], device, True, True)\n",
        "  plot_training_history(history)\n",
        "\n",
        "  test_model(model, training_data['test'], criterion, device, True, True)\n",
        "\n",
        "  wandb.log({\"Training Curves\": wandb.Image(\"training_plot.png\")})\n",
        "\n",
        "  torch.save(model.state_dict(), config['architecture'] + '.pt')\n",
        "  wandb.save(config['architecture'] + '.pt')  # upload to W&B"
      ],
      "metadata": {
        "id": "pjt6MpOgs2W5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xocKxhqawml_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
